{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3e7968-75d8-4361-a1cd-2b08860dfa51",
   "metadata": {},
   "source": [
    "# Efficient Requirement Analysis & Specification with LLM Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2fb97-c07b-405f-8909-546003d20977",
   "metadata": {},
   "source": [
    "## 🛠️ Config\n",
    "\n",
    "In this section, we configure the environment, suppress warnings, and load essential packages. The dotenv library allows us to securely load sensitive environment variables from a .env file, keeping credentials and other configuration details safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792af6c0-461a-4d91-b0d2-e290fef27806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import warnings  # To control warning outputs for a cleaner notebook\n",
    "warnings.filterwarnings('ignore')  # Suppress all warnings \n",
    "\n",
    "from dotenv import load_dotenv  # To load environment variables from a .env file\n",
    "\n",
    "import os  # To interact with the operating system\n",
    "import json  # For handling JSON data\n",
    "import yaml  # For handling YAML data which we will utilize for agent and task definition\n",
    "\n",
    "## Loading environment variables from the .env file\n",
    "load_dotenv()  # This will look for the .env file and load the variables\n",
    "\n",
    "## Alternatively, we can set the environment variables directly using os.environ\n",
    "# Example:\n",
    "# os.environ['OPENAI_API_KEY'] = 'xxxxxx'\n",
    "\n",
    "# Defining the default model name for OpenAI\n",
    "os.environ['OPENAI_MODEL_NAME'] = 'gpt-4o-mini'  # Setting the model to be used in subsequent operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959a74e-d071-4ed9-9a69-50ed279a52d6",
   "metadata": {},
   "source": [
    "## Single Agent, Single Task Use Case\n",
    "\n",
    "In this setup, we follow a 1 agent, 1 task framework, where each agent is assigned a unique, specific task. This approach enhances focus and efficiency, ensuring that each agent performs optimally by concentrating on their specialized assignment.\n",
    "\n",
    "Here's a breakdown:\n",
    "- 🤖 Agent Definition: Each agent is carefully designed with a clear role, goal, and backstory that align with their unique strengths and expertise. This alignment allows the agent to approach tasks with a deep understanding of their responsibilities and objectives. For more information, refer to the full list of [agent's attributes](https://docs.crewai.com/concepts/agents#agent-attributes).\n",
    "- ✅ Task Specification: Tasks are explicitly tailored to each agent’s capabilities, with well-defined description and expected_output attributes. This clarity allows agents to work independently, producing high-quality deliverables without requiring additional guidance. Refer to the complete list of [task's attributes](https://docs.crewai.com/concepts/tasks#task-attributes) for detailed task configuration.\n",
    "\n",
    "🔋 This setup creates a streamlined workflow by establishing a one-to-one mapping between agents and tasks, minimizing overlap and redundancy. Each agent focuses solely on their assigned task, resulting in a structured, organized, and effective process that promotes high-quality outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1da41-bd6c-4d73-ba5b-8f4ec3c121f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for YAML configurations\n",
    "files = {\n",
    "    'agents': 'config/agents.yaml',\n",
    "    'tasks': 'config/tasks.yaml'\n",
    "}\n",
    "\n",
    "# Load configurations from YAML files\n",
    "configs = {}\n",
    "for config_type, file_path in files.items():\n",
    "    with open(file_path, 'r') as file:\n",
    "        configs[config_type] = yaml.safe_load(file)\n",
    "\n",
    "# Assign loaded configurations to specific variables\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87869b41-de98-4e45-9da4-cbcac55fbbe6",
   "metadata": {},
   "source": [
    "### 🧰 Tools Definition\n",
    "\n",
    "Our toolkit includes a blend of CrewAI, Langchain, LlamaIndex, and custom tools designed for specialized search and retrieval tasks.\n",
    "\n",
    "#### 🔍 Search Tools\n",
    "\n",
    "- 🌐 **SerperDevTool**: Searches the internet, returning the most relevant Google results using the Serper API.\n",
    "\n",
    "- 🕸️ **WebsiteSearchTool**: Conducts Retrieval-Augmented Generation (RAG) searches within specific website content.\n",
    "\n",
    "- 📚 **ArxivSearchTool**: Leverages Langchain's wrapper for ArxivAPI to locate and retrieve research papers from arxiv.org.\n",
    "\n",
    "- 🤖 **Custom PerplexitySearchTool**: This tool utilizes Perplexity AI's API to perform AI-driven searches, returning relevant and contextually generated responses. \n",
    "\n",
    "#### ⛏️ Retrieval Tools\n",
    "\n",
    "- 📄 **Summary Index**: Extracts summaries from documents, storing both the summary and all associated document nodes.\n",
    "\n",
    "-  🔗 **Semantic Index**: Uses the Vector Store Index to create vector embeddings of document nodes, enabling semantic LLM queries.\n",
    "\n",
    "📋 These are our core tools so far. Feel free to suggest any additional tools you think would enhance this toolkit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59b5cc-6fe6-446b-aade-7ee822d64024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary search tools from CrewAI\n",
    "from crewai_tools import (\n",
    "    SerperDevTool,         # Tool for web searches using Google results via the Serper API\n",
    "    WebsiteSearchTool      # Tool for RAG (Retrieval-Augmented Generation) searches within specific website content\n",
    ")\n",
    "\n",
    "# Import BaseTool for creating custom tools and the tool decorator for additional flexibility\n",
    "from crewai_tools import BaseTool, tool\n",
    "\n",
    "# Import the arXiv API wrapper from Langchain's community utilities for academic research searches\n",
    "from langchain_community.utilities import ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8d490-dc90-4bbc-ae24-8db3ccfd166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a langchain tool for searching research papers on arXiv\n",
    "class ArxivSearchTool(BaseTool):\n",
    "    name: str = \"Arxiv Research Search Tool\"  \n",
    "    description: str = \"Searches arXiv for research papers.\" \n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Executes an arXiv search for research papers related to the given query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query string for arXiv.\n",
    "            \n",
    "        Returns:\n",
    "            str: A formatted list of top search results related to the query.\n",
    "        \"\"\"\n",
    "        arxiv_search = ArxivAPIWrapper(top_k_results=10)  # Limit search results to top 10 entries\n",
    "        search_result = arxiv_search.run(query)  # Run the search\n",
    "        return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712993d-5ca0-4249-974f-382b1e86cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom search tool using Perplexity AI's API for AI-driven search responses\n",
    "import requests\n",
    "from typing import ClassVar\n",
    "\n",
    "class PerplexitySearchTool(BaseTool):\n",
    "    name: str = \"Perplexity AI Search Tool\"\n",
    "    description: str = (\n",
    "        \"Executes searches using Perplexity AI for AI-driven information retrieval. \"\n",
    "        \"Queries should be passed as 'content' in natural language.\"\n",
    "    )\n",
    "\n",
    "    def _run(self, content: str) -> dict:\n",
    "        \"\"\"\n",
    "        Executes a search query via Perplexity AI and returns the AI-generated response.\n",
    "\n",
    "        Args:\n",
    "            content (str): The search query or question in natural language.\n",
    "\n",
    "        Returns:\n",
    "            dict: JSON response from Perplexity AI containing the search results.\n",
    "        \"\"\"\n",
    "        api_key: ClassVar[str] = os.environ[\"PERPLEXITY_API_KEY\"]  # Retrieve Perplexity API key from environment\n",
    "        url = \"https://api.perplexity.ai/chat/completions\"  # API endpoint for Perplexity search\n",
    "\n",
    "        # Define the payload with search parameters\n",
    "        payload = {\n",
    "            \"model\": \"llama-3.1-sonar-huge-128k-online\",  # Specify the model used for response generation\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content  # Pass the search content from user input\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.5  # Control response variability\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",  # Authorization header with API key\n",
    "            \"Content-Type\": \"application/json\"     # Set content type for JSON\n",
    "        }\n",
    "\n",
    "        # Make a POST request to the API and return the JSON response\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9d0e8-0824-42d0-b61f-47be3c016e81",
   "metadata": {},
   "source": [
    "For more information on the API's usage and parameters, refer to the official [Perplexity API documentation](https://perplexity.mintlify.app/api-reference/chat-completions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3579b0-d43a-4fe1-9aa4-1a59ec33efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search tool set\n",
    "from crewai_tools import (\n",
    "    SerperDevTool,\n",
    "    WebsiteSearchTool\n",
    ")\n",
    "\n",
    "# Creating custom tools by either using the BaseTool class or the @tools decorators as follows:\n",
    "from crewai_tools import BaseTool, tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d150b7-9c33-4ead-9c4d-6509b1c57705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom langchain tool\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "\n",
    "class ArxivSearchTool(BaseTool):\n",
    "    name: str = \"Arxiv Research Search Tool\"\n",
    "    description: str = \"Searches arXiv for research papers.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "            Runs an arXiv search with the given query\n",
    "        \"\"\"\n",
    "        arxiv_search = ArxivAPIWrapper( \n",
    "            top_k_results = 10\n",
    "        )\n",
    "        search_result = arxiv_search.run(query)\n",
    "        return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971bd7d-bc99-49eb-bfd4-92aa5a399346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom perplexity ai search tool\n",
    "import requests\n",
    "from typing import ClassVar\n",
    "\n",
    "class PerplexitySearchTool(BaseTool):\n",
    "    name: str = \"Perplexity AI Search Tool\"\n",
    "    description: str = \"Pass the queries as the content and execute searches using Perplexity AI for AI-driven information retrieval.\"\n",
    "\n",
    "    def _run(self, content: str) -> dict:\n",
    "        \"\"\"\n",
    "            Runs a Perplexity AI search\n",
    "        \"\"\"\n",
    "        api_key: ClassVar[str] = os.environ[\"PERPLEXITY_API_KEY\"]\n",
    "        url = \"https://api.perplexity.ai/chat/completions\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.5\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "        return response.json()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc1cfa-b3b6-4865-b77c-58b356b1dfd2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1af0d2-4fd0-4b80-83af-8abe0a192425",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722f2cdd-404f-4889-b0e0-b1712adb30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential modules and classes for creating a RAG (Retrieval-Augmented Generation) pipeline using LlamaIndex\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,       # Utility to read documents from a directory, transforming files into searchable content\n",
    "    DocumentSummaryIndex,        # Index type that stores document summaries\n",
    "    VectorStoreIndex,            # Index to store vector embeddings of documents for similarity-based search\n",
    "    StorageContext,              # Utility container to manage and handle the storing of nodes, indices, and vectors\n",
    "    Settings,                    # General configuration settings for LlamaIndex operations\n",
    "    get_response_synthesizer     # Function to synthesize responses based on retrieved data\n",
    ")\n",
    "\n",
    "# Importing additional node parsers for flexible data processing\n",
    "from llama_index.core.node_parser import *\n",
    "\n",
    "# Importing ChromaDB, a vector database for managing and querying embeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings as ChromaSettings  # Chroma settings to configure database connection and storage\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore  # Vector store that uses ChromaDB for efficient retrieval\n",
    "\n",
    "# Importing OpenAI tools for embedding and language model interactions\n",
    "from llama_index.llms.openai import OpenAI    # Interface for using OpenAI models for text generation\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding    # Tool for generating embeddings from OpenAI models\n",
    "\n",
    "# Importing CrewAI's tool wrapper for LlamaIndex integrations\n",
    "from crewai_tools import LlamaIndexTool    # Tool for using LlamaIndex functionalities in CrewAI\n",
    "\n",
    "# Standard Python libraries for system interaction and logging\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Configuring logging to track warnings and higher-level messages\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)  # Set default logging level to WARNING\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))  # Add stream handler to ensure output to stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d11868-e8db-4f4d-8555-f9f2c11075a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store local persistent directory\n",
    "persist_vector_store_path = 'chromadb'\n",
    "\n",
    "class QueryEngineToolset:\n",
    "    def __init__(self, document_dir, model_name, api_key, collection_name, load_collection_status):\n",
    "        self.document_dir = document_dir\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key\n",
    "        self.collection_name = collection_name\n",
    "        self.load_collection_status = load_collection_status\n",
    "        \n",
    "        Settings.llm = OpenAI(model=self.model_name, api_key=self.api_key, temperature=0)\n",
    "        Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small', api_key=self.api_key)\n",
    "        Settings.text_splitter = SemanticSplitterNodeParser(\n",
    "            buffer_size=1,\n",
    "            breakpoint_percentile_threshold=95, \n",
    "            embed_model=Settings.embed_model\n",
    "        )\n",
    "\n",
    "        # Load documents\n",
    "        self.documents = self.load_documents(document_dir=self.document_dir)\n",
    "         # Initialize vector store client\n",
    "        self.chroma_client = self.initialize_vector_store_client()\n",
    "        self.summary_index = self.create_summary_index(\n",
    "            collection_name=self.collection_name, \n",
    "            load_collection_status=self.load_collection_status\n",
    "        )\n",
    "        self.semantic_search_index = self.create_vector_store_index(\n",
    "            collection_name=self.collection_name, \n",
    "            load_collection_status=self.load_collection_status\n",
    "        )\n",
    "    \n",
    "    def load_documents(self, document_dir):\n",
    "        \"\"\"\n",
    "            Load documents from the specified directory.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            reader = SimpleDirectoryReader(input_dir=document_dir, required_exts=['.pdf', '.docx', '.txt', '.md', 'mp3', '.mp4'])\n",
    "            documents = reader.load_data()\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading documents: {e}\")\n",
    "            return []\n",
    "        \n",
    "    def initialize_vector_store_client(self):\n",
    "        \"\"\"\n",
    "        Initialize the Chroma vector store client\n",
    "        \"\"\"\n",
    "        try:\n",
    "            chroma_client = chromadb.PersistentClient(\n",
    "                path=persist_vector_store_path, \n",
    "                settings=ChromaSettings(allow_reset=True)\n",
    "            )\n",
    "            return chroma_client\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error iinitializing vector store client: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def initialize_chromadb_collection(self, chroma_client, collection_name, load_collection_status):\n",
    "        \"\"\"\n",
    "        Create or load a Chroma DB collection for storing vectors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            collections_list = [col.name for col in chroma_client.list_collections()]\n",
    "            if load_collection_status:\n",
    "                try:\n",
    "                    # Load existing collection\n",
    "                    if collection_name in collections_list:\n",
    "                        chromadb_collection = chroma_client.get_collection(name=collection_name)\n",
    "                        logging.info(f\"Loaded existing Chroma DB collection: {collection_name}\")\n",
    "                    else:\n",
    "                        logging.error(f\"Collection {collection_name} does not exist.\")\n",
    "                        return None\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error loading existing Chroma DB collection: {e}\")\n",
    "                    return None\n",
    "\n",
    "            else:\n",
    "                # Delete existing collection\n",
    "                if collection_name in collections_list:\n",
    "                    try:\n",
    "                        chroma_client.delete_collection(name=collection_name)\n",
    "                        logging.info(f\"Deleted existing Chroma DB collection: {collection_name}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error deleting existing Chroma DB collection: {e}\")\n",
    "                        return None\n",
    "                \n",
    "                # Create new collection\n",
    "                try:\n",
    "                    chromadb_collection = chroma_client.create_collection(name=collection_name)\n",
    "                    logging.info(f\"Created new Chroma DB collection: {collection_name}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error creating Chroma DB collection: {e}\")\n",
    "                    return None\n",
    "            \n",
    "            vector_store_instance = ChromaVectorStore(chroma_collection=chromadb_collection)\n",
    "            logging.info(f\"Chroma DB collection created or loaded successfully: {collection_name}\")\n",
    "            return vector_store_instance\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating Chroma DB collection: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_summary_index(self, collection_name, load_collection_status):\n",
    "        \"\"\"\n",
    "            Create a summary index for the loaded documents.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Creating or loading summary vector store with collection name: {collection_name}\")\n",
    "            summary_vector_store_instance = self.initialize_chromadb_collection(\n",
    "                chroma_client=self.chroma_client,\n",
    "                collection_name=f\"{collection_name}-summary\",\n",
    "                load_collection_status=self.load_collection_status\n",
    "            )\n",
    "            summary_storage_context = StorageContext.from_defaults(vector_store=summary_vector_store_instance)\n",
    "            response_synthesizer = get_response_synthesizer(\n",
    "                llm=Settings.llm, response_mode=\"tree_summarize\", use_async=True\n",
    "            )\n",
    "\n",
    "            if load_collection_status==True:\n",
    "                logging.info(\"Loading Document summary index from existing vector store\")\n",
    "                summary_index =  VectorStoreIndex.from_vector_store(\n",
    "                    vector_store=summary_vector_store_instance,\n",
    "                    storage_context=summary_storage_context\n",
    "                )\n",
    "            else:\n",
    "                logging.info(\"Creating DocumentSummaryIndex from documents\")\n",
    "                summary_index = DocumentSummaryIndex.from_documents(\n",
    "                    documents=self.documents,\n",
    "                    llm=Settings.llm,\n",
    "                    transformations=[Settings.text_splitter],\n",
    "                    response_synthesizer=response_synthesizer,\n",
    "                    storage_context=summary_storage_context,\n",
    "                    embed_model=Settings.embed_model,\n",
    "                    show_progress=True,\n",
    "                )\n",
    "            logging.info(\"DocumentSummaryIndex created successfully\")\n",
    "            return summary_index\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating summary index: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_vector_store_index(self, collection_name, load_collection_status):\n",
    "        \"\"\"\n",
    "        Create a vector store index for semantic search.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Creating or loading semantic search vector store with collection name: {collection_name}\")\n",
    "            semantic_search_vector_store_instance = self.initialize_chromadb_collection(\n",
    "                self.chroma_client,\n",
    "                collection_name=f\"{collection_name}-semantic_search\",\n",
    "                load_collection_status=self.load_collection_status\n",
    "            )\n",
    "\n",
    "            semantic_search_storage_context = StorageContext.from_defaults(\n",
    "                vector_store=semantic_search_vector_store_instance\n",
    "                )\n",
    "\n",
    "            if load_collection_status==True:\n",
    "                logging.info(\"Loading vector store index from existing vector store\")\n",
    "                vector_store_index = VectorStoreIndex.from_vector_store(\n",
    "                    vector_store=semantic_search_vector_store_instance,\n",
    "                    storage_context=semantic_search_storage_context\n",
    "                )\n",
    "            else:\n",
    "                logging.info(\"Creating vector store index from documents\")\n",
    "                vector_store_index = VectorStoreIndex.from_documents(\n",
    "                    documents=self.documents,\n",
    "                    llm=Settings.llm,\n",
    "                    transformations=[Settings.text_splitter],\n",
    "                    embed_model=Settings.embed_model,\n",
    "                    storage_context=semantic_search_storage_context,\n",
    "                    show_progress=True,\n",
    "                )\n",
    "            logging.info(\"Vector store index created successfully\")\n",
    "            return vector_store_index\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating vector store index: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_tools(self):\n",
    "        \"\"\"\n",
    "        Create query engine tools for interacting with the summary and vector store indices.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.summary_query_engine = self.summary_index.as_query_engine(response_mode=\"tree_summarize\", use_async=True, streaming=True)\n",
    "            self.vector_store_query_engine = self.semantic_search_index.as_query_engine(similarity_top_k=5, llm=Settings.llm, streaming=True)\n",
    "            self.summary_tool = LlamaIndexTool.from_query_engine(\n",
    "                self.summary_query_engine,\n",
    "                name=\"Summary Index Query Tool\",\n",
    "                description=\"Use this tool to query summaries over the given document(s).\"\n",
    "            )\n",
    "            self.vector_store_tool = LlamaIndexTool.from_query_engine(\n",
    "                self.vector_store_query_engine,\n",
    "                name=\"Vector Store Index Query Tool\",\n",
    "                description=\"Use this tool to semantic search over the given document(s).\"\n",
    "            )\n",
    "            return self.summary_tool, self.vector_store_tool\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating query engine tools: {e}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00eb5dd-9dd0-4084-a3fd-eeeafcc314a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the GDPR RAG pipelines\n",
    "gdpr_query_engine_toolset = QueryEngineToolset(\n",
    "    document_dir=\"inputs/compliance/\",\n",
    "    model_name=\"gpt-4o\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    collection_name=\"gdpr\",\n",
    "    load_collection_status=False\n",
    ")\n",
    "gdpr_summary_tool, gdpr_semantic_search_tool = gdpr_query_engine_toolset.create_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78336c5e-fd84-4011-ba4c-91da51293a15",
   "metadata": {},
   "source": [
    "## Agents, Tasks and Crew Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9c99a-3c0f-450b-967c-6989905b492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ceb86-a0eb-4879-8d98-2de93dc2a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent creation\n",
    "requirements_elicitation_agent = Agent(\n",
    "    config=agents_config['requirements_elicitation_agent']\n",
    ")\n",
    "\n",
    "market_researcher_agent = Agent(\n",
    "    config=agents_config['market_researcher_agent'],\n",
    "    tools=[\n",
    "        SerperDevTool(), \n",
    "        WebsiteSearchTool(), \n",
    "        ArxivSearchTool(), \n",
    "        PerplexitySearchTool()\n",
    "    ]\n",
    ")\n",
    "\n",
    "requirement_developer_agent = Agent(\n",
    "    config=agents_config['requirement_developer_agent'],\n",
    ")\n",
    "\n",
    "compliance_specialist_agent = Agent(\n",
    "    config=agents_config['compliance_specialist_agent'],\n",
    "    tools=[\n",
    "        gdpr_summary_tool, \n",
    "        gdpr_semantic_search_tool\n",
    "    ]\n",
    ")\n",
    "\n",
    "quality_control_analyst_agent = Agent(\n",
    "    config=agents_config['quality_control_analyst_agent'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775a71a-4a32-405f-b98c-5efc283ec6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task creation\n",
    "requirement_elicitation_gathering_task = Task(\n",
    "    config=tasks_config['requirement_elicitation_gathering_task'],\n",
    "    agent=requirements_elicitation_agent\n",
    ")\n",
    "\n",
    "market_research_task = Task(\n",
    "    config=tasks_config['market_research_task'],\n",
    "    agent=market_researcher_agent,\n",
    ")\n",
    "\n",
    "requirements_development_task = Task(\n",
    "    config=tasks_config['requirements_development_task'],\n",
    "    agent=requirement_developer_agent\n",
    ")\n",
    "\n",
    "compliance_assessment_task = Task(\n",
    "    config=tasks_config['compliance_assessment_task'],\n",
    "    agent=compliance_specialist_agent\n",
    ")\n",
    "\n",
    "quality_assurance_task = Task(\n",
    "    config=tasks_config['quality_assurance_task'],\n",
    "    agent=quality_control_analyst_agent,\n",
    "    output_file='outputs/final_BRD_v1.md'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6338b-9f76-46d5-bf6c-7e5af9b32b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Crew\n",
    "requirement_engineering_crew = Crew(\n",
    "    agents=[\n",
    "        requirements_elicitation_agent,\n",
    "        market_researcher_agent,\n",
    "        requirement_developer_agent,\n",
    "        compliance_specialist_agent,\n",
    "        quality_control_analyst_agent\n",
    "    ],\n",
    "    tasks=[\n",
    "        requirement_elicitation_gathering_task,\n",
    "        market_research_task,\n",
    "        requirements_development_task,\n",
    "        compliance_assessment_task,\n",
    "        quality_assurance_task\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a9263-a550-44e1-8c50-5436b7233cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"inputs\": \"\"\"\n",
    "    Meeting Notes: First Requirements Elicitation for Solar Analytics Mobile Application\n",
    "    Date: September 22, 2024\n",
    "    Attendees:\n",
    "    - Product Owner (PO): Elizabeth Ogunyemi\n",
    "    - Developer (Dev): Aboze Brain\n",
    "    Meeting Start Time: 10:00 AM\n",
    "\n",
    "    Summary:\n",
    "    Elizabeth (Product Owner) introduced the vision for a mobile app that enables users to track, monitor, and analyze solar energy usage, aiming to provide data-driven recommendations for optimizing energy efficiency in Africa.\n",
    "\n",
    "    Key Discussion Points:\n",
    "    - Data Collection: The app will integrate with existing solar panel APIs for seamless data collection, with potential for future hardware integration.\n",
    "    - Real-Time Visualization: Users should monitor energy generation and consumption in real-time, with a responsive UI for both Android and iOS platforms.\n",
    "    - Platform Choice: Cross-platform options (e.g., Flutter or React Native) versus native (Swift for iOS, Kotlin for Android) were discussed, balancing speed, cost, and performance.\n",
    "    - Personalized Recommendations: Machine learning could provide personalized energy-saving tips based on usage data, starting with basic recommendations and evolving over time.\n",
    "    - User Experience: Consideration of data sync intervals for real-time insights without draining battery life. Notifications for significant energy events to improve user engagement.\n",
    "    - Next Steps: PO and Dev agreed to begin drafting technical requirements, prioritize usability testing, and gather early user feedback.\n",
    "\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd529b8-3077-4d47-b102-365722016e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requirement_engineering_crew.kickoff(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1950cd-1b96-4e14-aca1-5c57498e9934",
   "metadata": {},
   "source": [
    "## Single Agent, Multiple Tasks Use Case\n",
    "\n",
    "In this setup, we adopt a **single-agent, multi-task** framework, where one agent is equipped to manage several tasks. This structure capitalizes on the agent's flexibility and adaptability, enabling them to handle multiple responsibilities efficiently within a defined scope.\n",
    "\n",
    "🔋 **Enhanced Workflow**: With this setup, the workflow remains dynamic and streamlined, as one agent can oversee multiple related tasks without needing frequent reconfiguration running tasks with dependency (`context`) and parallel (`async_execution`). This centralization keeps tasks organized and cohesive, ensuring consistent quality across all deliverables.\n",
    "\n",
    "### Additional Implementations:\n",
    "- **LLM Integration**: Modify LLMs to power the agent, using CrewAI’s LiteLLM to access a range of LLMs (see [LiteLLM documentation](https://docs.litellm.ai/docs/providers)). For this setup, we’ll use Groq and Anthropic.\n",
    "- **Code writing agents**: Enable code execution for the agent, but it requires Docker to be install on safe mode.\n",
    "- **Planning Models**: Incorporate models to facilitate task planning.\n",
    "- **Memory**: Add memory functionality to enhance task continuity and context retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ddc8b0-9537-45ff-907a-7b0f8ca0f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Groq and Anthropic LLMs\n",
    "groq_llm = \"groq/llama-3.1-70b-versatile\"\n",
    "anthropic_llm = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "# Add DOCXSearchTool which is RAG tool designed for semantic searching within DOCX documents.\n",
    "from crewai_tools import DOCXSearchTool\n",
    "\n",
    "doc_search_tool = DOCXSearchTool(docx='inputs/OXUM.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72253b05-cea1-4080-b44d-1682d44566b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for YAML configurations\n",
    "files = {\n",
    "    'agents': 'config/agents_v2.yaml',\n",
    "    'tasks': 'config/tasks_v2.yaml'\n",
    "}\n",
    "\n",
    "# Load configurations from YAML files\n",
    "configs = {}\n",
    "for config_type, file_path in files.items():\n",
    "    with open(file_path, 'r') as file:\n",
    "        configs[config_type] = yaml.safe_load(file)\n",
    "\n",
    "# Assign loaded configurations to specific variables\n",
    "agents_config_v2 = configs['agents']\n",
    "tasks_config_v2 = configs['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fa308-a0f3-4630-a3c0-d9412d0d37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import FileReadTool\n",
    "from crewai_tools import CodeInterpreterTool\n",
    "csv_tool = FileReadTool(file_path='inputs/solar-energy-consumption-africa.csv')\n",
    "\n",
    "# Agent creation\n",
    "requirements_elicitation_agent = Agent(\n",
    "    config=agents_config_v2['requirements_elicitation_agent'],\n",
    "    # llm=groq_llm,\n",
    ")\n",
    "\n",
    "data_analysis_agent = Agent(\n",
    "    config=agents_config_v2['data_analysis_agent'],\n",
    "    allow_code_execution=True,\n",
    "    tools=[csv_tool],\n",
    "    llm=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "market_researcher_agent = Agent(\n",
    "    config=agents_config_v2['market_researcher_agent'],\n",
    "    tools=[SerperDevTool(), WebsiteSearchTool(), ArxivSearchTool(), PerplexitySearchTool(), doc_search_tool],\n",
    "     # llm=anthropic_llm,\n",
    ")\n",
    "\n",
    "requirement_developer_agent = Agent(\n",
    "    config=agents_config_v2['requirement_developer_agent'],\n",
    ")\n",
    "\n",
    "compliance_specialist_agent = Agent(\n",
    "    config=agents_config_v2['compliance_specialist_agent'],\n",
    "    tools=[gdpr_summary_tool, gdpr_semantic_search_tool]\n",
    ")\n",
    "\n",
    "quality_control_analyst_agent = Agent(\n",
    "    config=agents_config_v2['quality_control_analyst_agent'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60659c66-3af1-4324-9e9d-e2cc3cb1026b",
   "metadata": {},
   "source": [
    "> 🚨 **Note: Docker Desktop Settings (for Windows or Mac)**  \n",
    "> **Problem**: Docker Desktop on Windows or Mac can restrict access to host directories. The `/host_mnt` structure is specific to Docker Desktop, so Docker may not be configured to access this path by default.\n",
    ">  \n",
    "> ✅ **Solution**: Go to Docker Desktop settings and add `/Users/dir` (or the equivalent path) as a shared folder under **Resources > File Sharing**. Restart Docker Desktop after making this change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1729e1e-0309-4577-8931-a8da7739c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task creation\n",
    "brand_voice_review_task = Task(\n",
    "    config=tasks_config_v2['brand_voice_review_task'],\n",
    "    agent=requirements_elicitation_agent,\n",
    "    # async_execution=True # To run in parallel\n",
    ")\n",
    "\n",
    "requirement_elicitation_gathering_task = Task(\n",
    "    config=tasks_config_v2['requirement_elicitation_gathering_task'],\n",
    "    agent=requirements_elicitation_agent,\n",
    "    context=[brand_voice_review_task]\n",
    ")\n",
    "\n",
    "data_analysis_task = Task(\n",
    "    config=tasks_config_v2['data_analysis_task'],\n",
    "    agent=data_analysis_agent,\n",
    "    context=[requirement_elicitation_gathering_task],\n",
    ")\n",
    "\n",
    "market_research_task = Task(\n",
    "    config=tasks_config_v2['market_research_task'],\n",
    "    context=[requirement_elicitation_gathering_task, data_analysis_task],\n",
    "    agent=market_researcher_agent,  \n",
    ")\n",
    "\n",
    "requirements_development_task = Task(\n",
    "    config=tasks_config_v2['requirements_development_task'],\n",
    "    context=[requirement_elicitation_gathering_task, data_analysis_task, market_research_task],\n",
    "    agent=requirement_developer_agent\n",
    ")\n",
    "\n",
    "compliance_assessment_task = Task(\n",
    "    config=tasks_config_v2['compliance_assessment_task'],\n",
    "    context=[requirements_development_task], \n",
    "    agent=compliance_specialist_agent\n",
    ")\n",
    "\n",
    "quality_assurance_task = Task(\n",
    "    config=tasks_config_v2['quality_assurance_task'],\n",
    "    agent=quality_control_analyst_agent,\n",
    "    context=[requirements_development_task, compliance_assessment_task],\n",
    "    output_file='outputs/final_BRD_v2.md'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6265a182-6740-461a-9793-7e031814b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Crew\n",
    "requirement_engineering_crew = Crew(\n",
    "    agents=[\n",
    "        requirements_elicitation_agent,\n",
    "        data_analysis_agent,\n",
    "        market_researcher_agent,\n",
    "        requirement_developer_agent,\n",
    "        compliance_specialist_agent,\n",
    "        quality_control_analyst_agent\n",
    "    ],\n",
    "    tasks=[\n",
    "        brand_voice_review_task,\n",
    "        requirement_elicitation_gathering_task,\n",
    "        data_analysis_task,\n",
    "        market_research_task,\n",
    "        requirements_development_task,\n",
    "        compliance_assessment_task,\n",
    "        quality_assurance_task\n",
    "    ],\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    # planning=True,\n",
    "    # planning_llm='gpt-4o',\n",
    "    # # For hierarchical process\n",
    "    # Process=Process.hierarchical,\n",
    "    # manager_llm='gpt-4o',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc656a-dbc5-4950-92ea-6c9bd75c6442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = requirement_engineering_crew.kickoff(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23b113-d416-4b94-9920-a14f8cd438cd",
   "metadata": {},
   "source": [
    "## CrewAI Testing Feature  🚀\n",
    "\n",
    "Testing is vital to ensure your CrewAI crew is delivering as expected. CrewAI provides built-in testing functionality to evaluate crew performance effortlessly. CrewAI makes testing straightforward with a command-line option and an in-code function:\n",
    "\n",
    "1. **CLI Command**: Run `ccrewai test --n_iterations 5 --model gpt-4o` to initiate testing.\n",
    "2. **In-Code Method**: Use `crew.test()` directly within your code to test your crew's performance.\n",
    "\n",
    "Run these tests to get detailed performance metrics, ensuring your crew is optimized for excellence! ✨\n",
    "\n",
    "\n",
    "## CrewAI Training Feature  🚀\n",
    "Train your CrewAI agents by providing feedback and achieving consistent results. The training feature in CrewAI allows you to enhance your AI agents by integrating feedback and refining their performance.\n",
    "\n",
    "1. **CLI Command**: Run `crea train -n <n_iterations> <filename> (optional)` to initiate testing.\n",
    "2. **In-Code Method**: Use `crewai.train()` directly within your code to test your crew's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce363b-8c3d-46fc-8541-33e28dd735c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "requirement_engineering_crew.test(n_iterations=2, openai_model_name='gpt-4o')                                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f00cc-9bcb-4192-8477-38d17dcde29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "requirement_engineering_crew.train(n_iterations=1, filename='outputs/training.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3854a-8bde-456d-ab24-f25163480edd",
   "metadata": {},
   "source": [
    "🎯 Pro Tip: Regular testing and training ensure your crew is always ready to deliver exceptional performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041daade-491b-47ef-85fa-beb1c1c1b9c7",
   "metadata": {},
   "source": [
    "### Comparing new test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d5fb3-3eb8-498d-bc7a-70e38fe7cc1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "requirement_engineering_crew.test(n_iterations=2, openai_model_name='gpt-4o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
